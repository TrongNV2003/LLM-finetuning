defaults:
  - _self_
  - dataset: address-dataset
  - model: qwen3

training_arguments:
  optim: "adamw_torch_fused"
  bf16: true
  do_train: true
  do_eval: true
  do_predict: true
  output_dir: "./finetuning-checkpoints"
  num_train_epochs: 5
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 1
  eval_accumulation_steps: 1
  packing: true
  dataset_text_field: "text"
  eval_strategy: "epoch"
  save_strategy: "epoch"
  save_only_model: true
  save_total_limit: 1
  learning_rate: 2e-4
  lr_scheduler_type: "linear"
  warmup_ratio: 0.1
  weight_decay: 0.01
  logging_steps: 100
  tf32: true
  gradient_checkpointing: false
  disable_tqdm: true
  report_to: "mlflow"
  logging_dir: "./logs"
  load_best_model_at_end: true
  greater_is_better: true
  torch_compile: false
  overwrite_output_dir: true
  metric_for_best_model: "eval_rouge_l"
  logging_strategy: steps
  seed: 42

evaluation_arguments:
  max_concurrent_queries: 4
  max_new_tokens: 256
  temperature: 0.3
  top_p: 0.9
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true

seed: 42
token: null
logging:
  mlflow:
    experiment_name: "llm-finetuning"
